{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3ytyXW-7Q6o"
   },
   "source": [
    "# Setup\n",
    "-  Follow the setup instructions based on your preferred environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeJDFDo87Q6s"
   },
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  \n",
    "We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  \n",
    "To do this, simply:\n",
    "\n",
    "### Create a new conda environment\n",
    "```bash\n",
    "# Be sure to deactivate any active environments first\n",
    "conda create -n hw4 python=3.12.4\n",
    "```\n",
    "\n",
    "### Activate the conda environment\n",
    "```bash\n",
    "conda activate hw4\n",
    "```\n",
    "\n",
    "### Install the dependencies using the provided `requirements.txt`\n",
    "```bash\n",
    "pip install --no-cache-dir --ignore-installed -r requirements.txt\n",
    "```\n",
    "\n",
    "### Ensure that your notebook is in the same working directory as the `Handout`\n",
    "This can be achieved by:\n",
    "1. Physically moving the notebook into the handout directory.\n",
    "2. Changing the notebookâ€™s current working directory to the handout directory using the os.chdir() function.\n",
    "\n",
    "### Open the notebook and select the newly created environment from the kernel selector.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ hw4lib/\n",
    "â”œâ”€â”€ mytorch/\n",
    "â”œâ”€â”€ tests/\n",
    "â””â”€â”€ hw4_data_subset/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tub92oPW7Q6t"
   },
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5qfxCxq7l-f"
   },
   "source": [
    "### Step 1: Get your handout\n",
    "- See writeup for recommended approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0wRO-k-7Q6u"
   },
   "outputs": [],
   "source": [
    "# Example: My preferred approach\n",
    "import os\n",
    "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
    "os.environ['GITHUB_TOKEN'] = \"your_token_here\"\n",
    "\n",
    "GITHUB_USERNAME = \"your_username_here\"\n",
    "REPO_NAME       = \"your_repo_name_here\"\n",
    "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmxMiKjIYv2_"
   },
   "outputs": [],
   "source": [
    "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
    "!cd {REPO_NAME} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfOQStjw7Q6w"
   },
   "source": [
    "### Step 2: Install Dependencies\n",
    "- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.\n",
    "- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maupv9q17Q6w"
   },
   "outputs": [],
   "source": [
    "%pip install --no-deps -r IDL-HW4/requirements.txt\n",
    "import os\n",
    "os.kill(os.getpid(), 9) # NOTE: This will restart the your colab Python runtime (required)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Yj32DflNHuI"
   },
   "source": [
    "### Step 3: Obtain Data\n",
    "\n",
    "- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KeP-XgwYA_M3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "Warning: Failed to create the file /content/s25-hw4-data.zip: No such file or \n",
      "Warning: directory\n",
      "\n",
      "  0 11.9G    0 18992    0     0  29600      0   5d 00h --:--:--   5d 00h 29600\n",
      "curl: (23) Failure writing output to destination\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'du' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o /content/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data\n",
    "!unzip -q -o /content/s25-hw4-data.zip -d /content/hw4_data\n",
    "!rm -rf /content/s25-hw4-data.zip\n",
    "!du -h --max-depth=2 /content/hw4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2YwJ0hy7Q6x"
   },
   "source": [
    "### Step 4: Move to Handout Directory\n",
    "You must be within the handout directory for the library imports to work!\n",
    "\n",
    "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
    "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ hw4lib/\n",
    "â”œâ”€â”€ mytorch/\n",
    "â”œâ”€â”€ tests/\n",
    "â””â”€â”€ hw4_data_subset/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzYdDIxw7Q6x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to run the notebook on Kaggle, we would recommend against it. This assignment is more resource intensive and may run slower on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get your handout\n",
    "- See writeup for recommended approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: My preferred approach\n",
    "import os\n",
    "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
    "os.environ['GITHUB_TOKEN'] = \"your_token_here\"\n",
    "\n",
    "GITHUB_USERNAME = \"your_username_here\"\n",
    "REPO_NAME       = \"your_repo_name_here\"\n",
    "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
    "!cd {REPO_NAME} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Dependencies\n",
    "- Simply set the `Environment` setting in the notebook to `Always use latest environment`. No need to install anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Obtain Data\n",
    "\n",
    "#### âš ï¸ Important: Kaggle Users  \n",
    "If you are using Kaggle, **do not manually download the data!** The dataset is large and may exceed your available disk space. Instead, follow these steps to add the dataset directly to your notebook:\n",
    "\n",
    "1. Open your **Kaggle Notebook**.  \n",
    "2. Navigate to **Notebook â†’ Input**.  \n",
    "3. Click **Add Input**.  \n",
    "4. In the search bar, paste the following URL:  \n",
    "   ðŸ‘‰ [https://www.kaggle.com/datasets/cmu11785/s25-hw4-data](https://www.kaggle.com/datasets/cmu11785/s25-hw4-data)  \n",
    "5. Click the **âž• (plus sign)** to add the dataset to your notebook.  \n",
    "\n",
    "#### ðŸ“Œ Note:  \n",
    "This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Handout Directory\n",
    "You must be within the handout directory for the library imports to work!\n",
    "\n",
    "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
    "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ hw4lib/\n",
    "â”œâ”€â”€ mytorch/\n",
    "â”œâ”€â”€ tests/\n",
    "â””â”€â”€ hw4_data_subset/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQG51p6e7Q6x"
   },
   "source": [
    "## PSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get your handout\n",
    "- See writeup for recommended approaches.\n",
    "- If you use Remote - SSH to connect to Bridges2, you can upload the handout to your project directory and work from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: My preferred approach\n",
    "import os\n",
    "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
    "os.environ['GITHUB_TOKEN'] = \"your_token_here\"\n",
    "\n",
    "GITHUB_USERNAME = \"your_username_here\"\n",
    "REPO_NAME       = \"your_repo_name_here\"\n",
    "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
    "!cd {REPO_NAME} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM4OZo_G7Q6x"
   },
   "source": [
    "### Step 2: Setting Up Your Environment on Bridges2\n",
    "\n",
    "For this homework, we are providing a shared Conda environment for the entire class. Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SSH into Bridges2\n",
    "```bash\n",
    "ssh username@bridges2.psc.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Navigate to your Project Directory\n",
    "```bash\n",
    "cd $PROJECT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Anaconda Module\n",
    "```bash\n",
    "module load anaconda3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Activate the provided HW4 Environment\n",
    "```bash\n",
    "conda deactivate # First, deactivate any existing Conda environment\n",
    "conda activate /jet/home/psamal/hw_envs/idl_hw4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Request a Compute Node\n",
    "```bash\n",
    "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Re-activate Environment\n",
    "If your Conda environment was deactivated due to node allocation:\n",
    "```bash\n",
    "conda deactivate # First, deactivate any existing Conda environment\n",
    "conda activate /jet/home/psamal/hw_envs/idl_hw4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Start Jupyter Notebook\n",
    "Launch Jupyter Notebook:\n",
    "```bash\n",
    "jupyter notebook --no-browser --ip=0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 8. Connect to Jupyter Server\n",
    "\n",
    "You can now use your prefered way of connecting to the Jupyter Server. Your options should be covered in the docs linked in post 558 @ piazza.\n",
    "\n",
    "The following is my preferred way of connecting to the Jupyter Server:\n",
    "\n",
    "##### 8.1 Connect in VSCode\n",
    "I prefer uploading the notebook to PSC Bridges2 storage ($PROJECT directory) and then connecting to the Jupyter Server from there.\n",
    "1. Use Remote - SSH to connect to Bridges2 and navigate to your project directory.\n",
    "2. Upload the notebook to the project directory.\n",
    "3. Open the notebook in VSCode.\n",
    "4. Go to **Kernel** â†’ **Select Another Kernel** â†’ **Existing Jupyter Server**\n",
    "5. Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
    "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249` \n",
    "\n",
    "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get Data\n",
    "- `NOTE`: This will download and unzip data for both `HW4P1` and `HW4P2`\n",
    "- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data. \n",
    "  - Disk accesses are much faster than what you would get from `$PROJECT` storage\n",
    "  - `IT IS NOT PERSISTENT`\n",
    "- `NOTE`: Make sure you have completed the previous steps before running this cell.\n",
    "- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o $LOCAL/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data\n",
    "!unzip -q -o $LOCAL/s25-hw4-data.zip -d $LOCAL/hw4_data\n",
    "!rm -rf $LOCAL/s25-hw4-data.zip\\\n",
    "!du -h --max-depth=2 $LOCAL/hw4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Handout Directory\n",
    "Depending on the way you are running your notebook, you may or may not need to run this cell. As long as you are within the handout directory for the library imports to work!\n",
    "\n",
    "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
    "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ hw4lib/\n",
    "â”œâ”€â”€ mytorch/\n",
    "â”œâ”€â”€ tests/\n",
    "â””â”€â”€ hw4_data_subset/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to the handout directory if you are not there already\n",
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhPm0t5d7Q6z"
   },
   "source": [
    "# Imports\n",
    "\n",
    "- If your setup was done correctly, you should be able to run the following cell without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torchmetrics) (1.24.4)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torchmetrics) (2.0.1+cpu)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ruthv\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0.0->torchmetrics) (1.2.1)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YAJF1-E87Q6z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    LMDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    CausalMask,\n",
    "    PadMask,\n",
    "    PositionalEncoding,\n",
    "    DecoderOnlyTransformer\n",
    ")\n",
    "from hw4lib.utils import (\n",
    "    create_optimizer,\n",
    "    create_scheduler,\n",
    "    plot_lr_schedule\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    LMTrainer,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "import wandb\n",
    "import yaml\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annotated-types==0.7.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting appnope\n",
      "  Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
      "Collecting asttokens\n",
      "  Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting beautifulsoup4==4.13.3\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Collecting certifi==2024.12.14\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Collecting charset-normalizer==3.4.0\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Collecting click==8.1.8\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting comm\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting contourpy==1.3.0\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Collecting cycler==0.12.1\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting debugpy\n",
      "  Downloading debugpy-1.8.13-cp39-cp39-win_amd64.whl (5.2 MB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Collecting docker-pycreds==0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting executing\n",
      "  Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting filelock==3.16.1\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting fonttools==4.55.3\n",
      "  Downloading fonttools-4.55.3-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting fsspec==2024.10.0\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Collecting gdown==5.2.0\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Collecting gitdb==4.0.12\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Collecting GitPython==3.1.44\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Collecting huggingface-hub==0.27.0\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Collecting idna==3.10\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting importlib_metadata\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "Collecting ipython\n",
      "  Downloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
      "Collecting jedi\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting Jinja2==3.1.4\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_client\n",
      "  Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Collecting jupyter_core\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Collecting kaggle==1.7.4.2\n",
      "  Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "Collecting kiwisolver==1.4.7\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Collecting lightning-utilities==0.12.0\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Collecting MarkupSafe==3.0.2\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Collecting matplotlib==3.9.4\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "Collecting matplotlib-inline\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Collecting mpmath==1.3.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting nest_asyncio\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==3.2.1\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting numpy==2.0.2\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting pandas==2.2.3\n",
      "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting parso\n",
      "  Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Collecting pexpect\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting pillow==11.0.0\n",
      "  Downloading pillow-11.0.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Collecting platformdirs\n",
      "  Downloading platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
      "Collecting prompt_toolkit\n",
      "  Downloading prompt_toolkit-3.0.50-py3-none-any.whl (387 kB)\n",
      "Collecting protobuf==5.29.3\n",
      "  Downloading protobuf-5.29.3-cp39-cp39-win_amd64.whl (434 kB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-7.0.0-cp37-abi3-win_amd64.whl (244 kB)\n",
      "Collecting ptyprocess\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pure_eval\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Collecting pydantic==2.10.5\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Collecting pydantic_core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting Pygments\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "Collecting pyparsing==3.2.0\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Collecting PySocks==1.7.1\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting pytz==2024.2\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Collecting PyYAML==6.0.2\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Collecting pyzmq\n",
      "  Downloading pyzmq-26.4.0-cp39-cp39-win_amd64.whl (644 kB)\n",
      "Collecting requests==2.32.3\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting seaborn==0.13.2\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting sentry-sdk==2.19.2\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Collecting setproctitle==1.3.4\n",
      "  Downloading setproctitle-1.3.4-cp39-cp39-win_amd64.whl (12 kB)\n",
      "Collecting setuptools==75.1.0\n",
      "  Downloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting six\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting smmap==5.0.2\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting soupsieve==2.6\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Collecting stack_data\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting tokenizers==0.21.0\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting torch==2.5.1\n",
      "  Downloading torch-2.5.1-cp39-cp39-win_amd64.whl (203.0 MB)\n",
      "Collecting torchaudio==2.5.1\n",
      "  Downloading torchaudio-2.5.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Collecting torchinfo==1.8.0\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting torchmetrics==1.6.1\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "Collecting tornado\n",
      "  Downloading tornado-6.4.2-cp38-abi3-win_amd64.whl (438 kB)\n",
      "Collecting tqdm==4.67.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pywin32_postinstall.exe and pywin32_testall.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pygmentize.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter-migrate.exe, jupyter-troubleshoot.exe and jupyter.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script slugify.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter-kernel.exe, jupyter-kernelspec.exe and jupyter-run.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts ipython.exe and ipython3.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script debugpy.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script wheel.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts wandb.exe and wb.exe are installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script kaggle.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gdown.exe is installed in 'C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "scipy 1.8.0 requires numpy<1.25.0,>=1.17.3, but you have numpy 2.0.2 which is incompatible.\n",
      "langchain 0.3.17 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
      "langchain-community 0.3.16 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
      "transformers 4.44.2 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.21.0 which is incompatible.\n",
      "torchvision 0.15.2+cpu requires torch==2.0.1, but you have torch 2.5.1 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.3 which is incompatible.\n",
      "spyder 5.1.5 requires jedi<0.19.0,>=0.17.2, but you have jedi 0.19.2 which is incompatible.\n",
      "spyder-kernels 2.1.3 requires jupyter-client<7,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "selenium 4.1.3 requires urllib3[secure,socks]~=1.26, but you have urllib3 2.2.3 which is incompatible.\n",
      "python-lsp-server 1.2.4 requires jedi<0.19.0,>=0.17.2, but you have jedi 0.19.2 which is incompatible.\n",
      "pinecone-client 5.0.1 requires pinecone-plugin-inference<2.0.0,>=1.0.3, but you have pinecone-plugin-inference 3.1.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 2.0.2 which is incompatible.\n",
      "node2vec 0.4.6 requires networkx<3.0,>=2.5, but you have networkx 3.2.1 which is incompatible.\n",
      "node2vec 0.4.6 requires numpy<2.0.0,>=1.19.5, but you have numpy 2.0.2 which is incompatible.\n",
      "moralis 0.1.41 requires python-dateutil~=2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "moralis 0.1.41 requires typing-extensions~=4.3.0, but you have typing-extensions 4.13.1 which is incompatible.\n",
      "moralis 0.1.41 requires urllib3~=1.26.7, but you have urllib3 2.2.3 which is incompatible.\n",
      "datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.4 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 3.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting traitlets\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Collecting typing_extensions\n",
      "  Downloading typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "Collecting tzdata==2024.2\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Collecting urllib3==2.2.3\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting wandb==0.19.2\n",
      "  Downloading wandb-0.19.2-py3-none-win_amd64.whl (19.7 MB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Collecting wheel==0.44.0\n",
      "  Downloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Collecting zipp\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting text-unidecode\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting eval-type-backport\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting pywin32>=300\n",
      "  Downloading pywin32-310-cp39-cp39-win_amd64.whl (9.6 MB)\n",
      "Installing collected packages: zipp, wcwidth, urllib3, typing-extensions, traitlets, smmap, six, pywin32, pure-eval, platformdirs, parso, numpy, mpmath, MarkupSafe, idna, executing, colorama, charset-normalizer, certifi, asttokens, webencodings, tzdata, tqdm, tornado, text-unidecode, sympy, stack-data, soupsieve, setuptools, requests, pyzmq, PyYAML, pytz, python-dateutil, PySocks, pyparsing, Pygments, pydantic-core, prompt-toolkit, pillow, packaging, networkx, matplotlib-inline, kiwisolver, jupyter-core, Jinja2, jedi, importlib-resources, importlib-metadata, gitdb, fsspec, fonttools, filelock, exceptiongroup, decorator, cycler, contourpy, annotated-types, torch, setproctitle, sentry-sdk, python-slugify, pydantic, ptyprocess, psutil, protobuf, pandas, nest-asyncio, matplotlib, lightning-utilities, jupyter-client, ipython, huggingface-hub, GitPython, eval-type-backport, docker-pycreds, debugpy, comm, click, bleach, beautifulsoup4, wheel, wandb, torchmetrics, torchinfo, torchaudio, tokenizers, seaborn, pickleshare, pexpect, kaggle, ipykernel, gdown, appnope\n",
      "Successfully installed GitPython-3.1.44 Jinja2-3.1.4 MarkupSafe-3.0.2 PySocks-1.7.1 PyYAML-6.0.2 Pygments-2.19.1 annotated-types-0.7.0 appnope-0.1.4 asttokens-3.0.0 beautifulsoup4-4.13.3 bleach-6.2.0 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.8 colorama-0.4.6 comm-0.2.2 contourpy-1.3.0 cycler-0.12.1 debugpy-1.8.13 decorator-5.2.1 docker-pycreds-0.4.0 eval-type-backport-0.2.2 exceptiongroup-1.2.2 executing-2.2.0 filelock-3.16.1 fonttools-4.55.3 fsspec-2024.10.0 gdown-5.2.0 gitdb-4.0.12 huggingface-hub-0.27.0 idna-3.10 importlib-metadata-8.6.1 importlib-resources-6.5.2 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.7.2 kaggle-1.7.4.2 kiwisolver-1.4.7 lightning-utilities-0.12.0 matplotlib-3.9.4 matplotlib-inline-0.1.7 mpmath-1.3.0 nest-asyncio-1.6.0 networkx-3.2.1 numpy-2.0.2 packaging-24.2 pandas-2.2.3 parso-0.8.4 pexpect-4.9.0 pickleshare-0.7.5 pillow-11.0.0 platformdirs-4.3.7 prompt-toolkit-3.0.50 protobuf-5.29.3 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pydantic-2.10.5 pydantic-core-2.27.2 pyparsing-3.2.0 python-dateutil-2.9.0.post0 python-slugify-8.0.4 pytz-2024.2 pywin32-310 pyzmq-26.4.0 requests-2.32.3 seaborn-0.13.2 sentry-sdk-2.19.2 setproctitle-1.3.4 setuptools-75.1.0 six-1.17.0 smmap-5.0.2 soupsieve-2.6 stack-data-0.6.3 sympy-1.13.1 text-unidecode-1.3 tokenizers-0.21.0 torch-2.5.1 torchaudio-2.5.1 torchinfo-1.8.0 torchmetrics-1.6.1 tornado-6.4.2 tqdm-4.67.1 traitlets-5.14.3 typing-extensions-4.13.1 tzdata-2024.2 urllib3-2.2.3 wandb-0.19.2 wcwidth-0.2.13 webencodings-0.5.1 wheel-0.44.0 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "pip install --user --no-cache-dir --ignore-installed -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q4Ccp-M7Q60"
   },
   "source": [
    "# Implementations\n",
    "\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vqefJri7Q60"
   },
   "source": [
    "## MyTorch Implementations\n",
    "- Modify your `Linear` implementation from HW1P1 to support arbitrary number of dimensions in `mytorch/nn/linear.py`.\n",
    "- Modify your `Softmax` implementation from HW1P1 to support arbitrary number of dimensions in `mytorch/nn/activation.py`.\n",
    "- Implement the `ScaledDotProductAttention` class in `mytorch/nn/scaled_dot_product_attention.py`.\n",
    "- Implement the `MultiHeadAttention` class in `mytorch/nn/multi_head_attention.py`.\n",
    "- Run the cell below to check your implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2hxzUzzW7Q60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: Linear\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Linear Tests\u001b[0m\n",
      "Testing Linear Layer ...\n",
      "Test Passed: Linear Forward\n",
      "Test Passed: Linear Backward\n",
      "\u001b[92m[01/01]    PASSED:   Linear Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: Softmax\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Softmax Tests\u001b[0m\n",
      "Testing Softmax ...\n",
      "Test Passed: Softmax Forward\n",
      "Test Passed: Softmax Backward\n",
      "\u001b[92m[01/01]    PASSED:   Softmax Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: ScaledDotProductAttention\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  ScaledDotProductAttention Tests\u001b[0m\n",
      "Testing Scaled Dot Product Attention ...\n",
      "Test Passed: Scaled Dot Product Attention Forward\n",
      "Test Passed: Scaled Dot Product Attention Backward\n",
      "\u001b[92m[01/01]    PASSED:   ScaledDotProductAttention Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: MultiHeadAttention\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  MultiHeadAttention Tests\u001b[0m\n",
      "Testing Multi Head Attention ...\n",
      "Test Passed: Multi Head Attention Forward\n",
      "Test Passed: Multi Head Attention Backward\n",
      "\u001b[92m[01/01]    PASSED:   MultiHeadAttention Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    Linear                        \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    Softmax                       \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    ScaledDotProductAttention     \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    MultiHeadAttention            \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_mytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8hb5VAN7Q60"
   },
   "source": [
    "## Dataset Implementation\n",
    "- Familiarize yourself with the `tokenize`, `encode`, and `decode` methods of the `H4Tokenizer` class in `hw4lib/data/tokenizer.py`. You will need to make use of these methods in both `HW4P1` and `HW4P2` both in the dataset implementations and during decoding.\n",
    "- Implement the `LMDataset` class in `hw4lib/data/lm_dataset.py`.\n",
    "    - You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods.\n",
    "- Run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "D1oCYvlJ7Q60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 192, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 140, in main\n",
      "    from hw4lib.data.lm_dataset import LMDataset\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\__init__.py\", line 5, in <module>\n",
      "    from .trainers import *\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\__init__.py\", line 1, in <module>\n",
      "    from .base_trainer import BaseTrainer\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\base_trainer.py\", line 6, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\__init__.py\", line 2, in <module>\n",
      "    from .rcmod import *  # noqa: F401,F403\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\rcmod.py\", line 5, in <module>\n",
      "    from . import palettes\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\palettes.py\", line 9, in <module>\n",
      "    from .utils import desaturate, get_color_cycle\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\utils.py\", line 11, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\__init__.py\", line 80, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\site-packages\\numexpr\\__init__.py\", line 26, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n",
      "AttributeError: _ARRAY_API not found\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 192, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 140, in main\n",
      "    from hw4lib.data.lm_dataset import LMDataset\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\__init__.py\", line 5, in <module>\n",
      "    from .trainers import *\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\__init__.py\", line 1, in <module>\n",
      "    from .base_trainer import BaseTrainer\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\base_trainer.py\", line 6, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\__init__.py\", line 2, in <module>\n",
      "    from .rcmod import *  # noqa: F401,F403\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\rcmod.py\", line 5, in <module>\n",
      "    from . import palettes\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\palettes.py\", line 9, in <module>\n",
      "    from .utils import desaturate, get_color_cycle\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\utils.py\", line 11, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\__init__.py\", line 80, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\site-packages\\bottleneck\\__init__.py\", line 2, in <module>\n",
      "    from .reduce import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\ruthv\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 192, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\tests\\test_dataset_lm.py\", line 140, in main\n",
      "    from hw4lib.data.lm_dataset import LMDataset\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\__init__.py\", line 5, in <module>\n",
      "    from .trainers import *\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\__init__.py\", line 1, in <module>\n",
      "    from .base_trainer import BaseTrainer\n",
      "  File \"C:\\Users\\ruthv\\Desktop\\IDL\\HW4\\IDL-HW4\\hw4lib\\trainers\\base_trainer.py\", line 6, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\__init__.py\", line 5, in <module>\n",
      "    from .relational import *  # noqa: F401,F403\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\relational.py\", line 21, in <module>\n",
      "    from ._statistics import EstimateAggregator, WeightedAggregator\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\_statistics.py\", line 32, in <module>\n",
      "    from scipy.stats import gaussian_kde\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\stats\\__init__.py\", line 453, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\stats\\_stats_py.py\", line 38, in <module>\n",
      "    from scipy.spatial.distance import cdist\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\spatial\\__init__.py\", line 102, in <module>\n",
      "    from ._kdtree import *\n",
      "  File \"C:\\Users\\ruthv\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\spatial\\_kdtree.py\", line 5, in <module>\n",
      "    from ._ckdtree import cKDTree, cKDTreeNode\n",
      "  File \"_ckdtree.pyx\", line 1, in init scipy.spatial._ckdtree\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_dataset_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6R08H787Q60"
   },
   "source": [
    "## Model Implementations\n",
    "#### Overview:\n",
    "- Implement the `CausalMask` and `PadMask` functions in `hw4lib/modules/masks.py` to handle masking.\n",
    "- Implement the `PositionalEncoding` class in `hw4lib/model/positional_encoding.py` to handle positional encoding.\n",
    "- Implement the Transformer Sublayers: `SelfAttentionLayer` and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- Implement the Transformer Layer: `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- Implement the `DecoderOnlyTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- Run the cells below to check your implementation.\n",
    "- `NOTE`: Besides the `DecoderOnlyTransformer` (P1 mandatory, P2 optional), you will use all of the above implementations in both `HW4P1` and `HW4P2`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6m-85zB7Q61"
   },
   "source": [
    "### Masks\n",
    "- Implement the `PadMask` and `CausalMask` functions in `hw4lib/modules/masks.py`.\n",
    "- Run the cell below to check your implementation.\n",
    "- You will need to make use of these masks in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di88pr8J7Q61"
   },
   "source": [
    "#### Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAHwBO7m7Q61"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_mask_causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCPEtI8X7Q61"
   },
   "source": [
    "#### Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiZusS-H7Q62"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_mask_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ZeV5XM7Q62"
   },
   "source": [
    "#### Optional: Visualize your Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UqY1xD_7Q62"
   },
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "_d_model   = 64\n",
    "_x         = torch.zeros(4, 20, _d_model)\n",
    "_x_len     = torch.tensor([5, 15, 10, 20])\n",
    "_x_causal  = CausalMask(_x)\n",
    "_x_padding = PadMask(_x, _x_len)\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, mask_axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot masks\n",
    "masks_and_titles = [\n",
    "    (_x_padding, \"Padding Mask\"),\n",
    "    (_x_causal, \"Causal Mask\")\n",
    "]\n",
    "\n",
    "# Plot each mask\n",
    "images = []\n",
    "for i, (mask, title) in enumerate(masks_and_titles):\n",
    "    im = mask_axs[i].imshow(mask, cmap=\"gray\", aspect='auto')\n",
    "    mask_axs[i].set_title(title, fontsize=8)\n",
    "    images.append(im)\n",
    "\n",
    "# Add colorbar at the bottom\n",
    "fig.subplots_adjust(bottom=0.2)  # Make space for colorbar\n",
    "cbar_ax = fig.add_axes([0.15, 0.1, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(images[0], cax=cbar_ax, orientation='horizontal')\n",
    "cbar.ax.set_xlabel('Mask Values', labelpad=5, fontsize=8)\n",
    "cbar.set_ticks([0, 1])\n",
    "cbar.set_ticklabels(['Attend (0)', 'Ignore/Mask (1)'])\n",
    "cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlkN0B1H7Q63"
   },
   "source": [
    "### Positional Encoding\n",
    "- Implement the `PositionalEncoding` class in `hw4lib/model/positional_encoding.py`.\n",
    "- Run the cell below to check your implementation.\n",
    "- You will need to make use of this positional encoding in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTO-EMdY7Q63"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehZGbq5G7Q64"
   },
   "source": [
    "#### Optional: Visualize your Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cgKYbz47Q64"
   },
   "outputs": [],
   "source": [
    "# Create sample positional encoding\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "pos_encoding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "pe = pos_encoding.pe.squeeze(0).numpy()  # Remove batch dimension and convert to numpy\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Positional encoding matrix\n",
    "im = ax1.imshow(pe, aspect='auto', cmap='RdBu',\n",
    "                extent=[0, d_model, max_len, 0])  # Flip y-axis to show position top-to-bottom\n",
    "plt.colorbar(im, ax=ax1, label='Encoding Value')\n",
    "ax1.set_xlabel('Dimension')\n",
    "ax1.set_ylabel('Position')\n",
    "ax1.set_title('Positional Encoding Matrix')\n",
    "ax1.grid(False)\n",
    "\n",
    "# Plot 2: Sinusoidal patterns\n",
    "dimensions = [0, 15, 31, 47, 63]  # Plot first few dimensions\n",
    "for dim in dimensions:\n",
    "    ax2.plot(pe[:, dim], label=f'dim {dim}')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Encoding Value')\n",
    "ax2.set_title('Sinusoidal Patterns for Different Dimensions')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSAUWGco7Q64"
   },
   "source": [
    "### Transformer Sublayers\n",
    "- Implement the Transformer Sublayers: `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- Run the cell below to check your implementation.\n",
    "- You will need to make use of all of these sublayers in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4auGdGYy7Q64"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_selfattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIy1d8757Q64"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfGn7MqL7Q65"
   },
   "source": [
    "### Transformer Self-Attention Decoder Layer\n",
    "- Implement the Transformer Layer: `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- Run the cell below to check your implementation.\n",
    "- You will need to make use of this sublayer in `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8p-8Uff7Q65"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_decoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0NW07hn7Q65"
   },
   "source": [
    "### Decoder-Only Transformer\n",
    "\n",
    "- Implement the `DecoderOnlyTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- Run the cell below to check your implementation.\n",
    "- You will need to make use of in `HW4P1` and optionally `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Z1SU9b97Q65"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_transformer_decoder_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSGdoo9J7Q65"
   },
   "source": [
    "## Decoding Implementation\n",
    "- Implement the `generate_greedy` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- Run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ks5wXPYS7Q65"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_decoding --mode greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Implementation\n",
    "You will have to do some minor in-filling for the `LMTrainer` class in `hw4lib/trainers/lm_trainer.py` before you can use it.\n",
    "- Fill in the `TODO`s in the `__init__`.\n",
    "- Fill in the `TODO`s in the `_train_epoch`.\n",
    "- Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- Fill in the `TODO`s in the `generate` method.\n",
    "- Fill in the `TODO`s in the `train` method.\n",
    "\n",
    "`WARNING`: There are no test's for this. Implement carefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2-j-cKa7Q65"
   },
   "source": [
    "# Experiments\n",
    "From this point onwards you may want to switch to a `GPU` runtime. \n",
    "- `OBJECTIVE`: You must achieve a per-character perplexity â‰¤ 3.5 in order to get points for Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV2GWmKd7Q65"
   },
   "source": [
    "## Config\n",
    "- You can use the `config.yaml` file to set your config for your ablation study.\n",
    "\n",
    "---\n",
    "### Notes:\n",
    "\n",
    "- Set `tokenization: token_type:` to specify your desired tokenization strategy\n",
    "- You will need to set the root path to your `hw4p1_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
    "  - `PSC`: `\"/local/hw4_data/hw4p1_data\"`\n",
    "  - `Colab:`: `\"/content/hw4_data/hw4p1_data\"`\n",
    "  - `Kaggle:`: `\"/kaggle/input/s25-hw4-data/hw4p1_data\"`\n",
    "- There's extra configurations in the `optimizer` section which will only be relevant if you decide to use the `create_optimizer` function we've provided in `hw4lib/utils/create_optimizer.py`.\n",
    "- `BE CAREFUL` while setting numeric values. Eg. `1e-4` will get serialized to a `str` while `1.0e-4` gets serialized to float. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLChmBx67Q65"
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Enter-Name-Here\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"char\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:                    # Currently setup for Colab assuming out setup\n",
    "  root                 : \"hw4_data/hw4p1_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train\"  # train\n",
    "  val_partition        : \"val\"    # val\n",
    "  test_partition       : \"test\"   # test\n",
    "  subset               : 1.0      # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 256      #\n",
    "  NUM_WORKERS          : 2        # Set to 0 for CPU\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Decoder-Only Language Model (HW4P1)\n",
    "  d_model                   : 256\n",
    "  d_ff                      : 1024\n",
    "  num_layers                : 2\n",
    "  num_heads                 : 2\n",
    "  dropout                   : 0.0\n",
    "  layer_drop_rate           : 0.0\n",
    "  weight_tying              : False\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : True   # Toggle wandb logging\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False  # Resume an existing run (run_id != 'none')\n",
    "  epochs                      : 20\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"Set-Project-Name-Here\" # wandb project to log to\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.0\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adam\" # Options: sgd, adam, adamw\n",
    "  lr: 5.0e-4   # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "  # Parameter groups\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing keywords set their learning rate to 0.0001\n",
    "      lr: 0.0001    # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: [] # Will match all parameters containing \"ffn\" and set their learning rate to 0.0001\n",
    "      lr: 0.0001   # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1.0e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 1.0e-8  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 4  # Number of iterations for the first restart\n",
    "    T_mult: 4  # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Fl_9Vv117Q66"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu56OILL7Q66"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBrysj6-7Q66"
   },
   "outputs": [],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'],\n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-vsGdfu7Q66"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCR3fGoL7Q66"
   },
   "outputs": [],
   "source": [
    "train_dataset  = LMDataset(\n",
    "    partition  = config['data']['train_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "val_dataset    = LMDataset(\n",
    "    partition  = config['data']['val_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "test_dataset   = LMDataset(\n",
    "    partition  = config['data']['test_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf8Y_COP7Q66"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0GajQ0LX7Q66"
   },
   "outputs": [],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn\n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLKjKWJQ7Q67"
   },
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY40PhyN7Q67"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Shfm0w7E7Q67"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlP3xkCg7Q67"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCrlXlZH7Q67"
   },
   "source": [
    "## Calculate Max Transcript Length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIGxOLDU7Q67"
   },
   "source": [
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models.\n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your language model to work with longer sequences in future tasks (hint: this might be useful for P2! ðŸ˜‰)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWAWHucJ7Q67"
   },
   "outputs": [],
   "source": [
    "max_transcript_length = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Global Max Transcript Length':<30} : {max_transcript_length}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot4OuRE27Q67"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL2-8IbL7Q67"
   },
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "model_config.update({\n",
    "    'max_len': max_transcript_length,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "model = DecoderOnlyTransformer(**model_config)\n",
    "\n",
    "# Get some inputs from the text loader\n",
    "for batch in train_loader:\n",
    "    shifted_transcripts, golden_transcripts, transcript_lengths = batch\n",
    "    print(\"Shape of shifted_transcripts : \", shifted_transcripts.shape)\n",
    "    print(\"Shape of golden_transcripts  : \", golden_transcripts.shape)\n",
    "    print(\"Shape of transcript_lengths  : \", transcript_lengths.shape)\n",
    "    break\n",
    "\n",
    "model_stats = summary(model, input_data=[shifted_transcripts, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH973f3x7Q67"
   },
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz7os7UA7Q68"
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"your_wandb_api_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQAemeOM7Q68"
   },
   "source": [
    "## Trainer\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    â””â”€â”€ {run_name}/\n",
    "        â”œâ”€â”€ config.yaml\n",
    "        â”œâ”€â”€ model_arch.txt\n",
    "        â”œâ”€â”€ checkpoints/\n",
    "        â”‚   â”œâ”€â”€ checkpoint-best-metric-model.pth\n",
    "        â”‚   â””â”€â”€ checkpoint-last-epoch-model.pth\n",
    "        â”œâ”€â”€ attn/\n",
    "        â”‚   â””â”€â”€ {attention visualizations}\n",
    "        â””â”€â”€ text/\n",
    "            â””â”€â”€ {generated text outputs}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcvGSnWi7Q68"
   },
   "outputs": [],
   "source": [
    "trainer = LMTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"test-lm\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJREaFhqiPrT"
   },
   "source": [
    "### Setup Optimizer and Scheduler\n",
    "\n",
    "You can set your own optimizer and scheduler by setting the class members in the `LMTrainer` class.\n",
    "Eg:\n",
    "```python\n",
    "trainer.optimizer = optim.AdamW(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])\n",
    "trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(trainer.optimizer, T_max=config['training']['epochs'])\n",
    "```\n",
    "\n",
    "We also provide a utility function to create your own optimizer and scheduler with the congig and some extra bells and whistles. You are free to use it or not. Do read their code and documentation to understand how it works (`hw4lib/utils/*`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkljGtIPkATt"
   },
   "outputs": [],
   "source": [
    "trainer.optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    opt_config=config['optimizer']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a test scheduler and plotting the learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inatJGBVi3II"
   },
   "outputs": [],
   "source": [
    "test_scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")\n",
    "\n",
    "plot_lr_schedule(\n",
    "    scheduler=test_scheduler,\n",
    "    num_epochs=config['training']['epochs'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXrwTbqdiPE_"
   },
   "outputs": [],
   "source": [
    "trainer.scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XI0dHJB7Q68"
   },
   "source": [
    "# Train\n",
    "- Set your epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nugAKoOw7Q68"
   },
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=config['training']['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j55r9gK_7Q68"
   },
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72D0yzHr7Q68"
   },
   "outputs": [],
   "source": [
    "test_metrics, test_generation_results = trainer.evaluate(test_loader)\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHw8LJp07Q68"
   },
   "source": [
    "# Submission\n",
    "To submit your assignment, you will need to create a `handin.tar` with the following directory structure:\n",
    "\n",
    "```\n",
    "handin/\n",
    "â”œâ”€â”€ mytorch/                     # Your implemented modules\n",
    "â”œâ”€â”€ test_metrics.json            # Results from evaluation\n",
    "â”œâ”€â”€ test_generated_results.json  # Sample text generations\n",
    "â””â”€â”€ model_arch.txt               # Model architecture summary\n",
    "```\n",
    "\n",
    "- Simply run the cell below once you are satisfied with your current state and this will create the `handin.tar` file.\n",
    "- After running the above cell, you should see the handin.tar file in the current directory\n",
    "- Upload the `handin.tar` file to the `HW4P1` assignment on Autolab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVSPaoOF7Q68"
   },
   "outputs": [],
   "source": [
    "# Create temporary handin directory\n",
    "if os.path.exists('handin'):\n",
    "    shutil.rmtree('handin')\n",
    "os.makedirs('handin')\n",
    "\n",
    "# Copy mytorch directory\n",
    "shutil.copytree('mytorch', 'handin/mytorch')\n",
    "\n",
    "# Save final results\n",
    "with open('handin/test_metrics.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "with open('handin/test_generated_results.json', 'w') as f:\n",
    "    json.dump(test_generation_results['greedy'], f, indent=4)\n",
    "\n",
    "# Save model architecture\n",
    "with open('handin/model_arch.txt', 'w') as f:\n",
    "    f.write(str(model_stats))\n",
    "\n",
    "# Create tar file with all exclusions handled by filter\n",
    "with tarfile.open('handin.tar', 'w') as tar:\n",
    "    def filter_files(tarinfo):\n",
    "        # Skip unwanted files\n",
    "        if any(pattern in tarinfo.name for pattern in [\n",
    "            '.DS_Store',\n",
    "            '__pycache__',\n",
    "            '.pyc'\n",
    "        ]):\n",
    "            return None\n",
    "        return tarinfo\n",
    "\n",
    "    tar.add('handin', arcname='handin', filter=filter_files)\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree('handin')\n",
    "\n",
    "print(\"Created handin.tar successfully!\")\n",
    "\n",
    "## After running the above cell, you should see the handin.tar file in the current directory\n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qQG51p6e7Q6x"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
