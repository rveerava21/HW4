{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksTZ5pAtXWXh"
      },
      "source": [
        "# README for HW4P2 Code Submission\n",
        "\n",
        "## Running the Code\n",
        "To run the code, follow these steps:\n",
        "1. Ensure you have all required dependencies installed. You can install them using:\n",
        "   bash\n",
        "   pip install -r requirements.txt\n",
        "   \n",
        "2. Run the main script:\n",
        "   bash\n",
        "   python main.py --config config.yaml\n",
        "   \n",
        "3. Modify the configuration file (config.yaml) to adjust hyperparameters, dataset paths, and model architecture.\n",
        "\n",
        "## Experiments and Architectures Tried\n",
        "I experimented using different tokenizers and architectures.\n",
        "\n",
        "\n",
        "## Training Details\n",
        "- Optimizer: AdamW\n",
        "- Learning Rate Scheduler: ReducedLROnPlateau, CosineAnnealing\n",
        "- Number of Epochs: 30 (best results obtained around epoch 12)\n",
        "- Batch Size: 6\n",
        "- Data Augmentation: Applied random cropping, flipping, and normalization\n",
        "- Loss Function: Cross-entropy loss\n",
        "\n",
        "\n",
        "## Best Hyperparameter Combination\n",
        "- Learning Rate: 5e-4 (with ReduceLROnPlateau)\n",
        "- Batch Size: 6\n",
        "\n",
        "## WanDB Logs\n",
        "All experiment logs, including training curves and performance metrics, are available on WandB. Access them using the link below:\n",
        "[WanDB Project Link](<https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2?nw=nwuserrveerava>)\n",
        "\n",
        "##Ablation Sheet\n",
        "https://docs.google.com/spreadsheets/d/1ETTGLLKQD2LL1K8afVoM-uC_DA3ZSAzU9i9JmRV9gPA/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "-  Follow the setup instructions based on your preferred environment!"
      ],
      "metadata": {
        "id": "Rx0IuaNoUp44"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhQJTlgXWXk"
      },
      "source": [
        "## Local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlykMP_XWXl"
      },
      "source": [
        "One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  \n",
        "We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  \n",
        "To do this, simply:\n",
        "\n",
        "### Create a new conda environment\n",
        "```bash\n",
        "# Be sure to deactivate any active environments first\n",
        "conda create -n hw4 python=3.12.4\n",
        "```\n",
        "\n",
        "### Activate the conda environment\n",
        "```bash\n",
        "conda activate hw4\n",
        "```\n",
        "\n",
        "### Install the dependencies using the provided `requirements.txt`\n",
        "```bash\n",
        "pip install --no-cache-dir --ignore-installed -r requirements.txt\n",
        "```\n",
        "\n",
        "### Ensure that your notebook is in the same working directory as the `Handout`\n",
        "This can be achieved by:\n",
        "1. Physically moving the notebook into the handout directory.\n",
        "2. Changing the notebook’s current working directory to the handout directory using the os.chdir() function.\n",
        "\n",
        "### Open the notebook and select the newly created environment from the kernel selector.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1K8ZJzQXWXm"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBCpeYGXWXm"
      },
      "source": [
        "### Step 1: Get your handout\n",
        "- See writeup for recommended approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CEuLMiFXWXm",
        "outputId": "9958a843-f1c3-4ebc-f871-6417042fca22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'HW4'...\n",
            "remote: Enumerating objects: 506, done.\u001b[K\n",
            "remote: Counting objects: 100% (506/506), done.\u001b[K\n",
            "remote: Compressing objects: 100% (440/440), done.\u001b[K\n",
            "remote: Total 506 (delta 62), reused 490 (delta 51), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (506/506), 27.21 MiB | 17.54 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ],
      "source": [
        "# Example: My preferred approach\n",
        "import os\n",
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"Your_Github_Token\"\n",
        "\n",
        "GITHUB_USERNAME = \"rveerava21\"\n",
        "REPO_NAME       = \"HW4\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "K7U9ItmLTYiw",
        "outputId": "a0abb158-1dfb-4faf-8a2f-780f8959d040"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'gc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-35e5c8a5ab93>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
          ]
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ov_XzmaXWXn",
        "outputId": "066f6857-b3c7-4875-f246-6f5f6e254895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MczkQoLuXWXo"
      },
      "source": [
        "### Step 2: Install Dependencies\n",
        "- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.\n",
        "- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBjx2u6Q7UU-",
        "outputId": "4538bec7-7d2d-4440-d0db-2b0f94ad1b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDL-HW4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_Mz1T577VBd",
        "outputId": "0454636f-2ed7-4f6f-8d9d-4c5470e7723b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.yaml  hw4_data_subset  mytorch\t\ttests\n",
            "expts\t     hw4lib\t      README.md\t\twandb\n",
            "hw4_data     HW4P1_nb.ipynb   requirements.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('IDL-HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5T2pSB57XTM",
        "outputId": "3243777d-7a7e-495c-9875-aecf69964719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hw4_data_subset  HW4P1_nb.ipynb  README.md\t   tests\n",
            "hw4lib\t\t mytorch\t requirements.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GVyqU62HXWXo",
        "outputId": "f0252a55-144b-4e77-eb4e-3bafdfbb2572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 1)) (0.7.0)\n",
            "Collecting appnope (from -r IDL-HW4/requirements.txt (line 2))\n",
            "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
            "Collecting asttokens (from -r IDL-HW4/requirements.txt (line 3))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting beautifulsoup4==4.13.3 (from -r IDL-HW4/requirements.txt (line 4))\n",
            "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting certifi==2024.12.14 (from -r IDL-HW4/requirements.txt (line 5))\n",
            "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting charset-normalizer==3.4.0 (from -r IDL-HW4/requirements.txt (line 6))\n",
            "  Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: click==8.1.8 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 7)) (8.1.8)\n",
            "Collecting comm (from -r IDL-HW4/requirements.txt (line 8))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting contourpy==1.3.0 (from -r IDL-HW4/requirements.txt (line 9))\n",
            "  Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: debugpy in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 11)) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 12)) (4.4.2)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 13)) (0.4.0)\n",
            "Collecting exceptiongroup (from -r IDL-HW4/requirements.txt (line 14))\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting executing (from -r IDL-HW4/requirements.txt (line 15))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting filelock==3.16.1 (from -r IDL-HW4/requirements.txt (line 16))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fonttools==4.55.3 (from -r IDL-HW4/requirements.txt (line 17))\n",
            "  Downloading fonttools-4.55.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2024.10.0 (from -r IDL-HW4/requirements.txt (line 18))\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gdown==5.2.0 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 19)) (5.2.0)\n",
            "Requirement already satisfied: gitdb==4.0.12 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 20)) (4.0.12)\n",
            "Requirement already satisfied: GitPython==3.1.44 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 21)) (3.1.44)\n",
            "Collecting huggingface-hub==0.27.0 (from -r IDL-HW4/requirements.txt (line 22))\n",
            "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 23)) (3.10)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 24)) (8.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 25)) (6.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 26)) (7.34.0)\n",
            "Collecting jedi (from -r IDL-HW4/requirements.txt (line 27))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting Jinja2==3.1.4 (from -r IDL-HW4/requirements.txt (line 28))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 29)) (6.1.12)\n",
            "Requirement already satisfied: jupyter_core in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 30)) (5.7.2)\n",
            "Requirement already satisfied: kaggle==1.7.4.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 31)) (1.7.4.2)\n",
            "Collecting kiwisolver==1.4.7 (from -r IDL-HW4/requirements.txt (line 32))\n",
            "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting lightning-utilities==0.12.0 (from -r IDL-HW4/requirements.txt (line 33))\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 34)) (3.0.2)\n",
            "Collecting matplotlib==3.9.4 (from -r IDL-HW4/requirements.txt (line 35))\n",
            "  Downloading matplotlib-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 36)) (0.1.7)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 37)) (1.3.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 38)) (1.6.0)\n",
            "Collecting networkx==3.2.1 (from -r IDL-HW4/requirements.txt (line 39))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 40)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 41)) (24.2)\n",
            "Collecting pandas==2.2.3 (from -r IDL-HW4/requirements.txt (line 42))\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 43)) (0.8.4)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 44)) (4.9.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 45)) (0.7.5)\n",
            "Collecting pillow==11.0.0 (from -r IDL-HW4/requirements.txt (line 46))\n",
            "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 47)) (4.3.7)\n",
            "Requirement already satisfied: prompt_toolkit in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 48)) (3.0.51)\n",
            "Collecting protobuf==5.29.3 (from -r IDL-HW4/requirements.txt (line 49))\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 50)) (5.9.5)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 51)) (0.7.0)\n",
            "Collecting pure_eval (from -r IDL-HW4/requirements.txt (line 52))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic==2.10.5 (from -r IDL-HW4/requirements.txt (line 53))\n",
            "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydantic_core==2.27.2 (from -r IDL-HW4/requirements.txt (line 54))\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 55)) (2.18.0)\n",
            "Collecting pyparsing==3.2.0 (from -r IDL-HW4/requirements.txt (line 56))\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 57)) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 58)) (2.8.2)\n",
            "Collecting pytz==2024.2 (from -r IDL-HW4/requirements.txt (line 59))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 60)) (6.0.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 61)) (24.0.1)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 62)) (2.32.3)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 63)) (0.13.2)\n",
            "Collecting sentry-sdk==2.19.2 (from -r IDL-HW4/requirements.txt (line 64))\n",
            "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting setproctitle==1.3.4 (from -r IDL-HW4/requirements.txt (line 65))\n",
            "  Downloading setproctitle-1.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting setuptools==75.1.0 (from -r IDL-HW4/requirements.txt (line 66))\n",
            "  Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 67)) (1.17.0)\n",
            "Requirement already satisfied: smmap==5.0.2 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 68)) (5.0.2)\n",
            "Requirement already satisfied: soupsieve==2.6 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 69)) (2.6)\n",
            "Collecting stack_data (from -r IDL-HW4/requirements.txt (line 70))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 71)) (1.13.1)\n",
            "Collecting tokenizers==0.21.0 (from -r IDL-HW4/requirements.txt (line 72))\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.5.1 (from -r IDL-HW4/requirements.txt (line 73))\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchaudio==2.5.1 (from -r IDL-HW4/requirements.txt (line 74))\n",
            "  Downloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchinfo==1.8.0 (from -r IDL-HW4/requirements.txt (line 75))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics==1.6.1 (from -r IDL-HW4/requirements.txt (line 76))\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 77)) (6.4.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 78)) (4.67.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 79)) (5.7.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 80)) (4.13.2)\n",
            "Collecting tzdata==2024.2 (from -r IDL-HW4/requirements.txt (line 81))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==2.2.3 (from -r IDL-HW4/requirements.txt (line 82))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting wandb==0.19.2 (from -r IDL-HW4/requirements.txt (line 83))\n",
            "  Downloading wandb-0.19.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 84)) (0.2.13)\n",
            "Collecting wheel==0.44.0 (from -r IDL-HW4/requirements.txt (line 85))\n",
            "  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from -r IDL-HW4/requirements.txt (line 86)) (3.21.0)\n",
            "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fonttools-4.55.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Downloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.19.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: torchaudio, stack_data, pytz, pure_eval, wheel, wandb, urllib3, tzdata, torchmetrics, torchinfo, torch, tokenizers, setuptools, setproctitle, sentry-sdk, pyparsing, pydantic_core, pydantic, protobuf, pillow, pandas, networkx, matplotlib, lightning-utilities, kiwisolver, Jinja2, jedi, huggingface-hub, fsspec, fonttools, filelock, executing, exceptiongroup, contourpy, comm, charset-normalizer, certifi, beautifulsoup4, asttokens, appnope\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.9\n",
            "    Uninstalling wandb-0.19.9:\n",
            "      Successfully uninstalled wandb-0.19.9\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: setproctitle\n",
            "    Found existing installation: setproctitle 1.3.5\n",
            "    Uninstalling setproctitle-1.3.5:\n",
            "      Successfully uninstalled setproctitle-1.3.5\n",
            "  Attempting uninstall: sentry-sdk\n",
            "    Found existing installation: sentry-sdk 2.26.1\n",
            "    Uninstalling sentry-sdk-2.26.1:\n",
            "      Successfully uninstalled sentry-sdk-2.26.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: pydantic_core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.30.2\n",
            "    Uninstalling huggingface-hub-0.30.2:\n",
            "      Successfully uninstalled huggingface-hub-0.30.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.57.0\n",
            "    Uninstalling fonttools-4.57.0:\n",
            "      Successfully uninstalled fonttools-4.57.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.2\n",
            "    Uninstalling contourpy-1.3.2:\n",
            "      Successfully uninstalled contourpy-1.3.2\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.4\n",
            "    Uninstalling beautifulsoup4-4.13.4:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.4\n",
            "Successfully installed Jinja2-3.1.4 appnope-0.1.4 asttokens-3.0.0 beautifulsoup4-4.13.3 certifi-2024.12.14 charset-normalizer-3.4.0 comm-0.2.2 contourpy-1.3.0 exceptiongroup-1.2.2 executing-2.2.0 filelock-3.16.1 fonttools-4.55.3 fsspec-2024.10.0 huggingface-hub-0.27.0 jedi-0.19.2 kiwisolver-1.4.7 lightning-utilities-0.12.0 matplotlib-3.9.4 networkx-3.2.1 pandas-2.2.3 pillow-11.0.0 protobuf-5.29.3 pure_eval-0.2.3 pydantic-2.10.5 pydantic_core-2.27.2 pyparsing-3.2.0 pytz-2024.2 sentry-sdk-2.19.2 setproctitle-1.3.4 setuptools-75.1.0 stack_data-0.6.3 tokenizers-0.21.0 torch-2.5.1 torchaudio-2.5.1 torchinfo-1.8.0 torchmetrics-1.6.1 tzdata-2024.2 urllib3-2.2.3 wandb-0.19.2 wheel-0.44.0\n"
          ]
        }
      ],
      "source": [
        "%pip install --no-deps -r IDL-HW4/requirements.txt\n",
        "import os\n",
        "os.kill(os.getpid(), 9) # NOTE: This will restart the your colab Python runtime (required)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dww5UXpXWXp"
      },
      "source": [
        "### Step 3: Obtain Data\n",
        "\n",
        "- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxYAzXm6FMfv",
        "outputId": "4aa6616f-0675-4339-8bea-e396fe5875b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface-hub<1.0,>=0.30.0\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.30.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.30.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.30.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.30.0) (2024.12.14)\n",
            "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/481.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.0\n",
            "    Uninstalling huggingface-hub-0.27.0:\n",
            "      Successfully uninstalled huggingface-hub-0.27.0\n",
            "Successfully installed huggingface-hub-0.30.2\n"
          ]
        }
      ],
      "source": [
        "pip install 'huggingface-hub>=0.30.0,<1.0' --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfPpo9nlXWXp",
        "outputId": "a3830dd3-df2e-44f4-f39e-e300bdcb2393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 11.9G  100 11.9G    0     0  40.6M      0  0:05:01  0:05:01 --:--:-- 40.9M\n",
            "757M\t/content/HW4/IDL-HW4/hw4_data/hw4p2_data/dev-clean\n",
            "14G\t/content/HW4/IDL-HW4/hw4_data/hw4p2_data/train-clean-100\n",
            "748M\t/content/HW4/IDL-HW4/hw4_data/hw4p2_data/test-clean\n",
            "16G\t/content/HW4/IDL-HW4/hw4_data/hw4p2_data\n",
            "28M\t/content/HW4/IDL-HW4/hw4_data/hw4p1_data/val\n",
            "28M\t/content/HW4/IDL-HW4/hw4_data/hw4p1_data/test\n",
            "1.1G\t/content/HW4/IDL-HW4/hw4_data/hw4p1_data/train\n",
            "1.1G\t/content/HW4/IDL-HW4/hw4_data/hw4p1_data\n",
            "17G\t/content/HW4/IDL-HW4/hw4_data\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o /content/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data\n",
        "!unzip -q -o /content/s25-hw4-data.zip -d /content/HW4/IDL-HW4/hw4_data\n",
        "!rm -rf /content/s25-hw4-data.zip\n",
        "!du -h --max-depth=2 /content/HW4/IDL-HW4/hw4_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV797kwAXWXp"
      },
      "source": [
        "### Step 4: Move to Handout Directory\n",
        "You must be within the handout directory for the library imports to work!\n",
        "\n",
        "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
        "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
        "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "mQOe2uDxXWXp",
        "outputId": "ade86b92-ea38-454c-c733-db6f72901d9e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'IDL-HW4'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3e9e79b48dd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IDL-HW4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IDL-HW4'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('IDL-HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqzcpqG0XWXp"
      },
      "source": [
        "## Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEg4SBYUXWXq"
      },
      "source": [
        "While it is possible to run the notebook on Kaggle, we would recommend against it. This assignment is more resource intensive and may run slower on Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMX5lqH9XWXq"
      },
      "source": [
        "### Step 1: Get your handout\n",
        "- See writeup for recommended approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qDr7vrLXWXq"
      },
      "outputs": [],
      "source": [
        "# Example: My preferred approach\n",
        "import os\n",
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
        "\n",
        "GITHUB_USERNAME = \"your_github_username_here\"\n",
        "REPO_NAME       = \"your_github_repo_name_here\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_CXP-GvXWXr"
      },
      "outputs": [],
      "source": [
        "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2nQNvB9XWXr"
      },
      "source": [
        "### Step 2: Install Dependencies\n",
        "- Simply set the `Environment` setting in the notebook to `Always use latest environment`. No need to install anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIu_QszEXWXr"
      },
      "source": [
        "### Step 3: Obtain Data\n",
        "\n",
        "#### ⚠️ Important: Kaggle Users  \n",
        "If you are using Kaggle, **do not manually download the data!** The dataset is large and may exceed your available disk space. Instead, follow these steps to add the dataset directly to your notebook:\n",
        "\n",
        "1. Open your **Kaggle Notebook**.  \n",
        "2. Navigate to **Notebook → Input**.  \n",
        "3. Click **Add Input**.  \n",
        "4. In the search bar, paste the following URL:  \n",
        "   👉 [https://www.kaggle.com/datasets/cmu11785/s25-hw4-data](https://www.kaggle.com/datasets/cmu11785/s25-hw4-data)  \n",
        "5. Click the **➕ (plus sign)** to add the dataset to your notebook.  \n",
        "\n",
        "#### 📌 Note:  \n",
        "This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eylVaaxrXWXr"
      },
      "source": [
        "### Step 4: Move to Handout Directory\n",
        "You must be within the handout directory for the library imports to work!\n",
        "\n",
        "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
        "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
        "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wpjv3ujXWXs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('IDL-HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP0Aucc7XWXs"
      },
      "source": [
        "## PSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJm_aSHvXWXs"
      },
      "source": [
        "### Step 1: Get your handout\n",
        "- See writeup for recommended approaches.\n",
        "- If you use Remote - SSH to connect to Bridges2, you can upload the handout to your project directory and work from there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simQXvJoXWXs"
      },
      "outputs": [],
      "source": [
        "# Example: My preferred approach\n",
        "import os\n",
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
        "\n",
        "GITHUB_USERNAME = \"your_github_username_here\"\n",
        "REPO_NAME       = \"your_github_repo_name_here\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ibi0yUFXWXs"
      },
      "outputs": [],
      "source": [
        "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXhjLh83XWXt"
      },
      "source": [
        "### Step 2: Setting Up Your Environment on Bridges2\n",
        "\n",
        "For this homework, we are providing a shared Conda environment for the entire class. Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG_taa71XWXt"
      },
      "source": [
        "#### 1. SSH into Bridges2\n",
        "```bash\n",
        "ssh username@bridges2.psc.edu\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hpcW2C5XWXt"
      },
      "source": [
        "#### 2. Navigate to your Project Directory\n",
        "```bash\n",
        "cd $PROJECT\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-YNBIVWXWXt"
      },
      "source": [
        "#### 3. Load the Anaconda Module\n",
        "```bash\n",
        "module load anaconda3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-7_f2PGXWXt"
      },
      "source": [
        "#### 4. Activate the provided HW4 Environment\n",
        "```bash\n",
        "conda deactivate # First, deactivate any existing Conda environment\n",
        "conda activate /jet/home/psamal/hw_envs/idl_hw4\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag3zHpx2XWXu"
      },
      "source": [
        "#### 5. Request a Compute Node\n",
        "```bash\n",
        "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "annERewlXWXu"
      },
      "source": [
        "#### 6. Re-activate Environment\n",
        "If your Conda environment was deactivated due to node allocation:\n",
        "```bash\n",
        "conda deactivate # First, deactivate any existing Conda environment\n",
        "conda activate /jet/home/psamal/hw_envs/idl_hw4\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPp6QNFGXWXu"
      },
      "source": [
        "#### 7. Start Jupyter Notebook\n",
        "Launch Jupyter Notebook:\n",
        "```bash\n",
        "jupyter notebook --no-browser --ip=0.0.0.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9QqEXMkXWXv"
      },
      "source": [
        "\n",
        "#### 8. Connect to Jupyter Server\n",
        "\n",
        "You can now use your prefered way of connecting to the Jupyter Server. Your options should be covered in the docs linked in post 558 @ piazza.\n",
        "\n",
        "The following is my preferred way of connecting to the Jupyter Server:\n",
        "\n",
        "##### 8.1 Connect in VSCode\n",
        "I prefer uploading the notebook to PSC Bridges2 storage ($PROJECT directory) and then connecting to the Jupyter Server from there.\n",
        "1. Use Remote - SSH to connect to Bridges2 and navigate to your project directory.\n",
        "2. Upload the notebook to the project directory.\n",
        "3. Open the notebook in VSCode.\n",
        "4. Go to **Kernel** → **Select Another Kernel** → **Existing Jupyter Server**\n",
        "5. Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
        "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
        "\n",
        "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTzqOnqVXWXw"
      },
      "source": [
        "### Step 3: Get Data\n",
        "- `NOTE`: This will download and unzip data for both `HW4P1` and `HW4P2`\n",
        "- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data.\n",
        "  - Disk accesses are much faster than what you would get from `$PROJECT` storage\n",
        "  - `IT IS NOT PERSISTENT`\n",
        "- `NOTE`: Make sure you have completed the previous steps before running this cell.\n",
        "- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx2lB7TKXWXw"
      },
      "outputs": [],
      "source": [
        "!curl -L -o $LOCAL/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data\n",
        "!unzip -q -o $LOCAL/s25-hw4-data.zip -d $LOCAL/hw4_data\n",
        "!rm -rf $LOCAL/s25-hw4-data.zip\\\n",
        "!du -h --max-depth=2 $LOCAL/hw4_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbFIgoFyXWXw"
      },
      "source": [
        "### Step 4: Move to Handout Directory\n",
        "Depending on the way you are running your notebook, you may or may not need to run this cell. As long as you are within the handout directory for the library imports to work!\n",
        "\n",
        "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
        "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
        "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmRtaGthXWXw"
      },
      "outputs": [],
      "source": [
        "# Move to the handout directory if you are not there already\n",
        "import os\n",
        "os.chdir('IDL-HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv-iljarXWXw"
      },
      "source": [
        "# Imports\n",
        "- If your setup was done correctly, you should be able to run the following cell without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41s27pCAXWXx",
        "outputId": "251ab843-eb55-4e78-937c-8a839bb59a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from hw4lib.data import (\n",
        "    H4Tokenizer,\n",
        "    ASRDataset,\n",
        "    verify_dataloader\n",
        ")\n",
        "from hw4lib.model import (\n",
        "    DecoderOnlyTransformer,\n",
        "    EncoderDecoderTransformer\n",
        ")\n",
        "from hw4lib.utils import (\n",
        "    create_scheduler,\n",
        "    create_optimizer,\n",
        "    plot_lr_schedule\n",
        ")\n",
        "from hw4lib.trainers import (\n",
        "    ASRTrainer,\n",
        "    ProgressiveTrainer\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import yaml\n",
        "import gc\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "import os\n",
        "import json\n",
        "import wandb\n",
        "import pandas as pd\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ey-Nd0XWXx"
      },
      "source": [
        "# Implementations\n",
        "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvQA7UchXWXx"
      },
      "source": [
        "## Dataset Implementation\n",
        "- Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`.\n",
        "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods.\n",
        "- Run the cell below to check your implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPZNGGJ9XWXx",
        "outputId": "5e224569-849a-45af-c207-2948f5b7a97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for train-clean-100 partition...\n",
            "100% 28/28 [00:00<00:00, 188.36it/s]\n",
            "Loading data for test-clean partition...\n",
            "100% 2/2 [00:00<00:00, 289.42it/s]\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: ASRDataset Train\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test a Train instance of ASRDataset class\u001b[0m\n",
            "Testing __init__ method ...\n",
            "Test Passed: Dataset length matches FBANK files.\n",
            "Test Passed: Dataset length matches TRANSCRIPT files.\n",
            "Test Passed: Order alignment between FBANK files and TRANSCRIPT files is correct.\n",
            "Test Passed: Alignment between features and transcripts is correct.\n",
            "Test Passed: All features have the correct number of dimensions (num_feats).\n",
            "Test Passed: All transcripts are decoded correctly after removing SOS and EOS tokens.\n",
            "Testing __getitem__ method ...\n",
            "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
            "Testing collate_fn method ...\n",
            "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
            "Test Passed: All sequences are padded to the same length.\n",
            "Test Passed: All transcripts are padded to the same length.\n",
            "Test Passed: Padding values are correct.\n",
            "\u001b[92m[01/01]    PASSED:   Test a Train instance of ASRDataset class\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: ASRDataset Test\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test a Test instance of ASRDataset class\u001b[0m\n",
            "Testing __init__ method ...\n",
            "Test Passed: Dataset length matches FBANK files.\n",
            "Test Passed: All features have the correct number of dimensions (num_feats).\n",
            "Testing __getitem__ method ...\n",
            "Test Passed: Transcripts are None for 'test-clean' at index 0.\n",
            "Test Passed: Transcripts are None for 'test-clean' at index 1.\n",
            "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
            "Testing collate_fn method ...\n",
            "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
            "Test Passed: All sequences are padded to the same length.\n",
            "Test Passed: Transcripts and lengths are correctly set to None for 'test-clean'.\n",
            "\u001b[92m[01/01]    PASSED:   Test a Test instance of ASRDataset class\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    ASRDataset Train              \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
            "\u001b[93mCategory:    ASRDataset Test               \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_dataset_asr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te9keQC7YOLh",
        "outputId": "03e40116-2101-4740-8ddd-0f10d828e76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HW4  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTKyK49VXWXx"
      },
      "source": [
        "## Model Implementations\n",
        "\n",
        "Overview:\n",
        "\n",
        "- Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
        "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
        "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
        "- Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POb3Agw6XWXy"
      },
      "source": [
        "### Transformer Sublayers\n",
        "- Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
        "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
        "- Run the cell below to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vYRTVdGXWXy",
        "outputId": "9fe9cf8b-1f55-494f-a95b-5c672a41a070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: CrossAttentionLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the cross-attention sublayer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All layers exist and are instantiated correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward pass returns the correct shapes\n",
            "Testing padding mask behaviour ...\n",
            "Test Passed: Padding mask is applied correctly\n",
            "Testing cross-attention behaviour ...\n",
            "Test Passed: Cross-attention behavior is correct\n",
            "Testing residual connection ...\n",
            "Test Passed: Residual connection is applied correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the cross-attention sublayer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    CrossAttentionLayer           \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_sublayer_crossattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbIAaQ--XWXy"
      },
      "source": [
        "### Transformer Cross-Attention Decoder Layer\n",
        "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
        "- Then run the cell below to check your implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Ke3vAVXWXy",
        "outputId": "1edfdc74-fa9f-4990-e03e-ee4575bed4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: CrossAttentionDecoderLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the cross-attention decoder layer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All sublayers exist and are initialized correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward shapes are as expected\n",
            "Testing sublayer integration ...\n",
            "Test Passed: Sublayers interact correctly\n",
            "Testing cross-attention behavior ...\n",
            "Test Passed: Cross-attention behaves correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the cross-attention decoder layer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    CrossAttentionDecoderLayer    \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_decoderlayer_crossattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28snypxrXWXy"
      },
      "source": [
        "### Transformer Self-Attention Encoder Layer\n",
        "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
        "- Then run the cell below to check your implementation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_bRj56mXWXy",
        "outputId": "6e3e54d7-0bfb-478c-d527-3716e005f2e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: SelfAttentionEncoderLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the self-attention encoder layer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All sublayers exist and are initialized correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward shapes are as expected\n",
            "Testing sublayer interaction ...\n",
            "Test Passed: Sublayers interact correctly\n",
            "Testing bidirectional attention ...\n",
            "Test Passed: Bidirectional attention is working correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the self-attention encoder layer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    SelfAttentionEncoderLayer     \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_encoderlayer_selfattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b9Co8HXWXy"
      },
      "source": [
        "### Encoder-Decoder Transformer\n",
        "\n",
        "- Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
        "- Then run the cell below to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ACVZWXrXWXy",
        "outputId": "b1f2d494-5fd1-4245-dbc8-a33e5a6b59a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: EncoderDecoderTransformer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the encoder-decoder transformer\u001b[0m\n",
            "Testing initialization...\n",
            "Test Passed: All components initialized correctly\n",
            "Testing encode method...\n",
            "Test Passed: Encode method works correctly\n",
            "Testing decode method...\n",
            "Test Passed: Decode method works correctly\n",
            "Testing forward pass...\n",
            "Test Passed: Forward pass works correctly\n",
            "Testing encoder-decoder integration...\n",
            "Test Passed: Encoder-decoder integration works correctly\n",
            "Testing CTC integration...\n",
            "Test Passed: CTC integration works correctly\n",
            "Testing forward propagation order...\n",
            "Test Passed: Forward propagation order is correct\n",
            "\u001b[92m[01/01]    PASSED:   Test the encoder-decoder transformer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    EncoderDecoderTransformer     \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_transformer_encoder_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxMaLdHdXWXy"
      },
      "source": [
        "## Decoding Implementation\n",
        "- We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
        "- Then run the cell below to check your implementation.\n",
        "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnAQO_VHXWXy",
        "outputId": "4d894acf-9ce1-4604-9242-b873b01519ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: Decoding\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test beam decoding\u001b[0m\n",
            "Testing Single Batch Beam Search ...\n",
            "Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
            "Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
            "Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
            "Testing Multi Batch Beam Search ...\n",
            "Batch 0  : Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
            "Batch 0  : Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
            "Batch 0  : Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
            "Batch 1  : Beam 0  : generated: GOOD BYE     | expected: GOOD BYE    \n",
            "Batch 1  : Beam 1  : generated: GREAT DAY    | expected: GREAT DAY   \n",
            "Batch 1  : Beam 2  : generated: GUD NIGHT    | expected: GUD NIGHT   \n",
            "\u001b[92m[01/01]    PASSED:   Test beam decoding\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    Decoding                      \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_decoding --mode beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze6Ufzt2XWXy"
      },
      "source": [
        "## Trainer Implementation\n",
        "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
        "- Fill in the `TODO`s in the `__init__`.\n",
        "- Fill in the `TODO`s in the `_train_epoch`.\n",
        "- Fill in the `TODO`s in the `recognize` method.\n",
        "- Fill in the `TODO`s in the `_validate_epoch`.\n",
        "- Fill in the `TODO`s in the `train` method.\n",
        "- Fill in the `TODO`s in the `evaluate` method.\n",
        "\n",
        "`WARNING`: There are no test's for this. Implement carefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CtOMbVXWXz"
      },
      "source": [
        "# Experiments\n",
        "From this point onwards you may want to switch to a `GPU` runtime.\n",
        "- `OBJECTIVE`: Optimize your model for `CER` on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-P4qGdjXWXz"
      },
      "source": [
        "## Config\n",
        "- You can use the `config.yaml` file to set your config for your ablation study.\n",
        "\n",
        "---\n",
        "### Notes:\n",
        "\n",
        "- Set `tokenization: token_type:` to specify your desired tokenization strategy\n",
        "- You will need to set the root path to your `hw4p1_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
        "  - `PSC`: `\"/local/hw4_data/hw4p1_data\"`\n",
        "  - `Colab:`: `\"/content/hw4_data/hw4p1_data\"`\n",
        "  - `Kaggle:`: `\"/kaggle/input/s25-hw4-data/hw4p1_data\"`\n",
        "- There's extra configurations in the `optimizer` section which will only be relevant if you decide to use the `create_optimizer` function we've provided in `hw4lib/utils/create_optimizer.py`.\n",
        "- `BE CAREFUL` while setting numeric values. Eg. `1e-4` will get serialized to a `str` while `1.0e-4` gets serialized to float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkFcCs6VXWXz",
        "outputId": "85f3d957-9eaa-4649-e701-c821ec45d53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "Name                      : \"Ruthvik Veeravalli\"\n",
        "\n",
        "###### Tokenization ------------------------------------------------------------\n",
        "tokenization:\n",
        "  token_type                : \"char\"       # [char, 1k, 5k, 10k]\n",
        "  token_map :\n",
        "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
        "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
        "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
        "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
        "\n",
        "###### Dataset -----------------------------------------------------------------\n",
        "data:\n",
        "  root                 : \"hw4_data/hw4p2_data\"  # TODO: Set the root path of your data\n",
        "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
        "  val_partition        : \"dev-clean\"        # paired text-speech for ASR pre-training\n",
        "  test_partition       : \"test-clean\"       # paired text-speech for ASR pre-training\n",
        "  subset               : 1.0              # Load a subset of the data (for debugging, testing, etc\n",
        "  batch_size           : 6           #\n",
        "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
        "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
        "  num_feats            : 80\n",
        "\n",
        "  ###### SpecAugment ---------------------------------------------------------------\n",
        "  specaug                   : True  # Set to True if you want to use SpecAugment\n",
        "  specaug_conf:\n",
        "    apply_freq_mask         : True\n",
        "    freq_mask_width_range   : 5\n",
        "    num_freq_mask           : 4\n",
        "    apply_time_mask         : True\n",
        "    time_mask_width_range   : 40\n",
        "    num_time_mask           : 4\n",
        "\n",
        "###### Network Specs -------------------------------------------------------------\n",
        "model: # Encoder-Decoder Transformer (HW4P2)\n",
        "  # Speech embedding parameters\n",
        "  input_dim: 80              # Speech feature dimension\n",
        "  time_reduction: 2          # Time dimension downsampling factor\n",
        "  reduction_method: 'lstm'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
        "\n",
        "  # Architecture parameters\n",
        "  d_model: 384          # Model dimension\n",
        "  num_encoder_layers: 10  # Number of encoder layers\n",
        "  num_decoder_layers: 6  # Number of decoder layers\n",
        "  num_encoder_heads: 8   # Number of encoder attention heads\n",
        "  num_decoder_heads: 8   # Number of decoder attention heads\n",
        "  d_ff_encoder: 768     # Feed-forward dimension for encoder\n",
        "  d_ff_decoder: 1536     # Feed-forward dimension for decoder\n",
        "  skip_encoder_pe: True # Whether to skip positional encoding for encoder\n",
        "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
        "\n",
        "  # Common parameters\n",
        "  dropout: 0.2       # Dropout rate\n",
        "  layer_drop_rate: 0.1  # Layer dropout rate\n",
        "  weight_tying: False   # Whether to use weight tying\n",
        "\n",
        "###### Common Training Parameters ------------------------------------------------\n",
        "training:\n",
        "  use_wandb                   : True   # Toggle wandb logging\n",
        "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
        "  resume                      : True   # Resume an existing run (run_id != 'none')\n",
        "  gradient_accumulation_steps : 1\n",
        "  wandb_project               : \"HW4P2\" # wandb project to log to\n",
        "\n",
        "###### Loss ----------------------------------------------------------------------\n",
        "loss: # Just good ol' CrossEntropy\n",
        "  label_smoothing: 0.1\n",
        "  ctc_weight: 0.2\n",
        "\n",
        "###### Optimizer -----------------------------------------------------------------\n",
        "optimizer:\n",
        "  name: \"adamw\" # Options: sgd, adam, adamw\n",
        "  lr: 0.0005    # Base learning rate\n",
        "\n",
        "  # Common parameters\n",
        "  weight_decay: 0.000001\n",
        "\n",
        "  # Parameter groups\n",
        "  # You can add more param groups as you want and set their learning rates and patterns\n",
        "  param_groups:\n",
        "    - name: self_attn\n",
        "      patterns: []  # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
        "      lr: 0.0002    # LR for self_attn\n",
        "      layer_decay:\n",
        "        enabled: False\n",
        "        decay_rate: 0.8\n",
        "\n",
        "    - name: ffn\n",
        "      patterns: [] # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
        "      lr: 0.0002   # LR for ffn\n",
        "      layer_decay:\n",
        "        enabled: False\n",
        "        decay_rate: 0.8\n",
        "\n",
        "  # Layer-wise learning rates\n",
        "  layer_decay:\n",
        "    enabled: False\n",
        "    decay_rate: 0.75\n",
        "\n",
        "  # SGD specific parameters\n",
        "  sgd:\n",
        "    momentum: 0.9\n",
        "    nesterov: True\n",
        "    dampening: 0\n",
        "\n",
        "  # Adam specific parameters\n",
        "  adam:\n",
        "    betas: [0.9, 0.999]\n",
        "    eps: 1.0e-8\n",
        "    amsgrad: False\n",
        "\n",
        "  # AdamW specific parameters\n",
        "  adamw:\n",
        "    betas: [0.9, 0.999]\n",
        "    eps: 1.0e-8\n",
        "    amsgrad: False\n",
        "\n",
        "###### Scheduler -----------------------------------------------------------------\n",
        "scheduler:\n",
        "  name: \"cosine_warm\"  # Options: reduce_lr, cosine, cosine_warm\n",
        "\n",
        "  # ReduceLROnPlateau specific parameters\n",
        "  reduce_lr:\n",
        "    mode: \"min\"  # Options: min, max\n",
        "    factor: 0.1  # Factor to reduce learning rate by\n",
        "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
        "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
        "    threshold_mode: \"rel\"  # Options: rel, abs\n",
        "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
        "    min_lr: 0.0000001  # Minimum learning rate\n",
        "    eps: 1e-8  # Minimal decay applied to lr\n",
        "\n",
        "  # CosineAnnealingLR specific parameters\n",
        "  cosine:\n",
        "    T_max: 15  # Maximum number of iterations\n",
        "    eta_min: 0.0000001  # Minimum learning rate\n",
        "    last_epoch: -1\n",
        "\n",
        "  # CosineAnnealingWarmRestarts specific parameters\n",
        "  cosine_warm:\n",
        "    T_0: 10    # Number of iterations for the first restart\n",
        "    T_mult: 10 # Factor increasing T_i after each restart\n",
        "    eta_min: 0.0000001  # Minimum learning rate\n",
        "    last_epoch: -1\n",
        "\n",
        "  # Warmup parameters (can be used with any scheduler)\n",
        "  warmup:\n",
        "    enabled: True\n",
        "    type: \"exponential\"  # Options: linear, exponential\n",
        "    epochs: 5\n",
        "    start_factor: 0.1\n",
        "    end_factor: 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycLt56sWXWXz"
      },
      "outputs": [],
      "source": [
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq8X3BcuXWXz"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gstAD2BGXWXz",
        "outputId": "b2021b06-613e-438e-ef67-bf7a783e3877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                         Tokenizer Configuration (char)                         \n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary size:     35\n",
            "\n",
            "Special Tokens:\n",
            "PAD:              0\n",
            "UNK:              1\n",
            "MASK:             2\n",
            "SOS:              3\n",
            "EOS:              4\n",
            "BLANK:            5\n",
            "\n",
            "Validation Example:\n",
            "--------------------------------------------------------------------------------\n",
            "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
            "Tokens:      ['[SOS]', 'H', 'I', ' ', 'D', 'E', 'E', 'P', ' ', 'L', 'E', 'A', 'R', 'N', 'E', 'R', 'S', '[EOS]']\n",
            "Token IDs:   [3, 13, 12, 6, 16, 7, 7, 25, 6, 17, 7, 9, 15, 11, 7, 15, 14, 4]\n",
            "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "Tokenizer = H4Tokenizer(\n",
        "    token_map  = config['tokenization']['token_map'],\n",
        "    token_type = config['tokenization']['token_type']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCLVwnaXWXz"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq5oSH6xJ--D",
        "outputId": "ed038dee-9944-44a5-d45d-2ac409290e8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwmZydebXWXz",
        "outputId": "8a31efb7-c7b6-46c9-c327-dda3a07eefe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for train-clean-100 partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28539/28539 [01:15<00:00, 376.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global stats computed from training set.\n",
            "Loading data for dev-clean partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2703/2703 [00:01<00:00, 1366.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for test-clean partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2620/2620 [00:00<00:00, 2786.78it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1694"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = ASRDataset(\n",
        "    partition=config['data']['train_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=True,\n",
        "    global_stats=None  # Will compute stats from training data\n",
        ")\n",
        "\n",
        "# TODO: Get the computed global stats from training set\n",
        "global_stats = None\n",
        "if config['data']['norm'] == 'global_mvn':\n",
        "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
        "    print(f\"Global stats computed from training set.\")\n",
        "\n",
        "val_dataset = ASRDataset(\n",
        "    partition=config['data']['val_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=False,\n",
        "    global_stats=global_stats\n",
        ")\n",
        "\n",
        "test_dataset = ASRDataset(\n",
        "    partition=config['data']['test_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=False,\n",
        "    global_stats=global_stats\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAFMrdMLXWXz"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGRynmFPXWXz",
        "outputId": "85d0b3b8-b3a1-4564-d47f-0f757fe4aa9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader    = DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = True,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = train_dataset.collate_fn\n",
        ")\n",
        "\n",
        "val_loader      = DataLoader(\n",
        "    dataset     = val_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = val_dataset.collate_fn\n",
        ")\n",
        "\n",
        "test_loader     = DataLoader(\n",
        "    dataset     = test_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = test_dataset.collate_fn\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YILqrSuXWX0"
      },
      "source": [
        "### Dataloader Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awodg0EWXWX0",
        "outputId": "b525287a-8559-45eb-e5fb-70427246a735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : train-clean-100\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 4757\n",
            "Batch Size               : 6\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n",
            "Feature Shape            : [6, 1926, 80]\n",
            "Shifted Transcript Shape : [6, 300]\n",
            "Golden Transcript Shape  : [6, 300]\n",
            "Feature Lengths Shape    : [6]\n",
            "Transcript Lengths Shape : [6]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 3066\n",
            "Max Transcript Length    : 399\n",
            "Avg. Chars per Token     : 1.00\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPNijuHBXWX0",
        "outputId": "bce65e2d-afff-407e-d7be-d51575d3abdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : dev-clean\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 451\n",
            "Batch Size               : 6\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n",
            "Feature Shape            : [6, 3676, 80]\n",
            "Shifted Transcript Shape : [6, 363]\n",
            "Golden Transcript Shape  : [6, 363]\n",
            "Feature Lengths Shape    : [6]\n",
            "Transcript Lengths Shape : [6]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 4081\n",
            "Max Transcript Length    : 517\n",
            "Avg. Chars per Token     : 1.00\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYjQOBj2XWX0",
        "outputId": "5e214768-e302-44c9-a31e-928c640fe0b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : test-clean\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 437\n",
            "Batch Size               : 6\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n",
            "Feature Shape            : [6, 1305, 80]\n",
            "Feature Lengths Shape    : [6]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 4370\n",
            "Max Transcript Length    : 0\n",
            "Avg. Chars per Token     : 0.00\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8_T2fH3XWX0"
      },
      "source": [
        "## Calculate Max Lengths\n",
        "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models.\n",
        "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
        "- This maximum length is a hyperparameter that determines:\n",
        "  - How long of a sequence your model can process\n",
        "  - The size of your positional encoding matrix\n",
        "  - Memory requirements during training and inference\n",
        "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! 😉).\n",
        "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guY_wgFjXWX0",
        "outputId": "58131b46-b4f9-4a13-e0e0-b0a6488699be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Max Feature Length             : 4370\n",
            "Max Transcript Length          : 517\n",
            "Overall Max Length             : 4370\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
        "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
        "max_len            = max(max_feat_len, max_transcript_len)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
        "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
        "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmryM_AHXWX0"
      },
      "source": [
        "## Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fEQ0Ns_XWX0",
        "outputId": "289bf619-e017-4366-a580-62bfcda7a208"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrveerava\u001b[0m (\u001b[33mrveerava-carnegie-mellon-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"Your_Wandb_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXcjkHQ_XWX0"
      },
      "source": [
        "## Training\n",
        "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
        "```\n",
        "expts/\n",
        "    └── {run_name}/\n",
        "        ├── config.yaml\n",
        "        ├── model_arch.txt\n",
        "        ├── checkpoints/\n",
        "        │   ├── checkpoint-best-metric-model.pth\n",
        "        │   └── checkpoint-last-epoch-model.pth\n",
        "        ├── attn/\n",
        "        │   └── {attention visualizations}\n",
        "        └── text/\n",
        "            └── {generated text outputs}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS7fwKk5XWX0"
      },
      "source": [
        "### Training Strategy 1: Cold-Start Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-y9NyN4XWX0"
      },
      "source": [
        "#### Model Load (Default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTSjyngVXWX0",
        "outputId": "1d60c329-2466-4301-a02b-81d12c2a91c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "EncoderDecoderTransformer                          [6, 260, 35]              --\n",
            "├─SpeechEmbedding: 1-1                             [6, 923, 384]             --\n",
            "│    └─StackedBLSTMEmbedding: 2-1                  [6, 923, 384]             --\n",
            "│    │    └─LSTM: 3-1                              [9025, 384]               420,864\n",
            "│    │    └─MaxPool1d: 3-2                         [6, 384, 923]             --\n",
            "│    │    └─LSTM: 3-3                              [4511, 384]               887,808\n",
            "│    │    └─MaxPool1d: 3-4                         [6, 384, 923]             --\n",
            "│    │    └─Linear: 3-5                            [6, 923, 384]             147,840\n",
            "│    │    └─Dropout: 3-6                           [6, 923, 384]             --\n",
            "├─Dropout: 1-2                                     [6, 923, 384]             --\n",
            "├─ModuleList: 1-3                                  --                        --\n",
            "│    └─SelfAttentionEncoderLayer: 2-2              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-7                [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-8                  [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-3              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-9                [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-10                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-4              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-11               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-12                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-5              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-13               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-14                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-6              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-15               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-16                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-7              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-17               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-18                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-8              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-19               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-20                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-9              [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-21               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-22                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-10             [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-23               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-24                 [6, 923, 384]             591,744\n",
            "│    └─SelfAttentionEncoderLayer: 2-11             [6, 923, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-25               [6, 923, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-26                 [6, 923, 384]             591,744\n",
            "├─LayerNorm: 1-4                                   [6, 923, 384]             768\n",
            "├─Sequential: 1-5                                  [6, 923, 35]              --\n",
            "│    └─Linear: 2-12                                [6, 923, 35]              13,475\n",
            "│    └─LogSoftmax: 2-13                            [6, 923, 35]              --\n",
            "├─Embedding: 1-6                                   [6, 260, 384]             13,440\n",
            "├─PositionalEncoding: 1-7                          [6, 260, 384]             --\n",
            "├─Dropout: 1-8                                     [6, 260, 384]             --\n",
            "├─ModuleList: 1-9                                  --                        --\n",
            "│    └─CrossAttentionDecoderLayer: 2-14            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-27               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-28              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-29                 [6, 260, 384]             1,182,336\n",
            "│    └─CrossAttentionDecoderLayer: 2-15            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-30               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-31              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-32                 [6, 260, 384]             1,182,336\n",
            "│    └─CrossAttentionDecoderLayer: 2-16            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-33               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-34              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-35                 [6, 260, 384]             1,182,336\n",
            "│    └─CrossAttentionDecoderLayer: 2-17            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-36               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-37              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-38                 [6, 260, 384]             1,182,336\n",
            "│    └─CrossAttentionDecoderLayer: 2-18            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-39               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-40              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-41                 [6, 260, 384]             1,182,336\n",
            "│    └─CrossAttentionDecoderLayer: 2-19            [6, 260, 384]             --\n",
            "│    │    └─SelfAttentionLayer: 3-42               [6, 260, 384]             592,128\n",
            "│    │    └─CrossAttentionLayer: 3-43              [6, 260, 384]             592,128\n",
            "│    │    └─FeedForwardLayer: 3-44                 [6, 260, 384]             1,182,336\n",
            "├─LayerNorm: 1-10                                  [6, 260, 384]             768\n",
            "├─Linear: 1-11                                     [6, 260, 35]              13,475\n",
            "====================================================================================================\n",
            "Total params: 27,536,710\n",
            "Trainable params: 27,536,710\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.TERABYTES): 3.00\n",
            "====================================================================================================\n",
            "Input size (MB): 3.56\n",
            "Forward/backward pass size (MB): 1167.85\n",
            "Params size (MB): 58.11\n",
            "Estimated Total Size (MB): 1229.51\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "model_config = config['model'].copy()\n",
        "model_config.update({\n",
        "    'max_len': max_len,\n",
        "    'num_classes': Tokenizer.vocab_size\n",
        "})\n",
        "\n",
        "model = EncoderDecoderTransformer(**model_config)\n",
        "\n",
        "# Get some inputs from the train dataloader\n",
        "for batch in train_loader:\n",
        "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
        "    break\n",
        "\n",
        "\n",
        "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
        "print(model_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN6wc57eXWX1"
      },
      "source": [
        "#### Initialize Trainer\n",
        "\n",
        "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
        "\n",
        "```python\n",
        "checkpoint_path = \"path/to/checkpoint.pth\"\n",
        "trainer.load_checkpoint(checkpoint_path)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "1eg-NsHzXWX1",
        "outputId": "b5876faf-1aaa-46cc-95f6-f6958c43ded1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/HW4/IDL-HW4/wandb/run-20250423_042057-5xp2vh4b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2/runs/5xp2vh4b' target=\"_blank\">Hw4p2-2-try-cosinewarm-10-6</a></strong> to <a href='https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2' target=\"_blank\">https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2/runs/5xp2vh4b' target=\"_blank\">https://wandb.ai/rveerava-carnegie-mellon-university/HW4P2/runs/5xp2vh4b</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = ASRTrainer(\n",
        "    model=model,\n",
        "    tokenizer=Tokenizer,\n",
        "    config=config,\n",
        "    run_name=\"Hw4p2-2-try-cosinewarm-10-6\",\n",
        "    config_file=\"config.yaml\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQSYD7jXWX1"
      },
      "source": [
        "### Setup Optimizer and Scheduler\n",
        "\n",
        "You can set your own optimizer and scheduler by setting the class members in the `LMTrainer` class.\n",
        "Eg:\n",
        "```python\n",
        "trainer.optimizer = optim.AdamW(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])\n",
        "trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(trainer.optimizer, T_max=config['training']['epochs'])\n",
        "```\n",
        "\n",
        "We also provide a utility function to create your own optimizer and scheduler with the congig and some extra bells and whistles. You are free to use it or not. Do read their code and documentation to understand how it works (`hw4lib/utils/*`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_IQUFhmXWX1"
      },
      "source": [
        "#### Setting up the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1mMBVjrXWX1",
        "outputId": "01abf1e0-ce55-434a-a9e8-ccad305a3ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔧 Configuring Optimizer:\n",
            "├── Type: ADAMW\n",
            "├── Base LR: 0.0005\n",
            "├── Weight Decay: 1e-06\n",
            "├── Parameter Groups:\n",
            "│   ├── Group: self_attn\n",
            "│   │   ├── LR: 0.0002\n",
            "│   │   └── Patterns: []\n",
            "│   ├── Group: ffn\n",
            "│   │   ├── LR: 0.0002\n",
            "│   │   └── Patterns: []\n",
            "│   └── Default Group (unmatched parameters)\n",
            "└── AdamW Specific:\n",
            "    ├── Betas: [0.9, 0.999]\n",
            "    ├── Epsilon: 1e-08\n",
            "    └── AMSGrad: False\n"
          ]
        }
      ],
      "source": [
        "trainer.optimizer = create_optimizer(\n",
        "    model=model,\n",
        "    opt_config=config['optimizer']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h4o2qWEXWX1"
      },
      "source": [
        "#### Creating a test scheduler and plotting the learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ilPlvS6EXWX1",
        "outputId": "f186c1c4-ee02-4958-ee0d-ca09da1b10c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 Configuring Learning Rate Scheduler:\n",
            "├── Type: COSINE_WARM\n",
            "├── Cosine Annealing Warm Restarts Settings:\n",
            "│   ├── T_0: 10 epochs (47570 steps)\n",
            "│   ├── T_mult: 10\n",
            "│   └── Min LR: 1e-07\n",
            "├── Warmup Settings:\n",
            "│   ├── Duration: 5 epochs (23785 steps)\n",
            "│   ├── Start Factor: 0.1\n",
            "│   └── End Factor: 1.0\n",
            "Warning: Only showing 5 out of 255 parameter groups for clarity\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_scheduler = create_scheduler(\n",
        "    optimizer=trainer.optimizer,\n",
        "    scheduler_config=config['scheduler'],\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")\n",
        "\n",
        "plot_lr_schedule(\n",
        "    scheduler=test_scheduler,\n",
        "    num_epochs=20,\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qQK403XWX1"
      },
      "source": [
        "#### Setting up the scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3-MTweSXWX2",
        "outputId": "02e3e9e2-27a6-4d3b-f42e-5081bf2fed4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 Configuring Learning Rate Scheduler:\n",
            "├── Type: COSINE_WARM\n",
            "├── Cosine Annealing Warm Restarts Settings:\n",
            "│   ├── T_0: 10 epochs (47570 steps)\n",
            "│   ├── T_mult: 10\n",
            "│   └── Min LR: 1e-07\n",
            "├── Warmup Settings:\n",
            "│   ├── Duration: 5 epochs (23785 steps)\n",
            "│   ├── Start Factor: 0.1\n",
            "│   └── End Factor: 1.0\n"
          ]
        }
      ],
      "source": [
        "trainer.scheduler = create_scheduler(\n",
        "    optimizer=trainer.optimizer,\n",
        "    scheduler_config=config['scheduler'],\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y3ASKqWXWX2"
      },
      "source": [
        "#### Train\n",
        "- Set your epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKCzg3U8XWX2",
        "outputId": "00f40e05-520b-4b4c-e7ce-8f47ab1314af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 0):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 2.4653\n",
            "│   ├── ctc_loss: 3.1155\n",
            "│   ├── joint_loss: 3.0884\n",
            "│   ├── perplexity_char: 11.7673\n",
            "│   └── perplexity_token: 11.7673\n",
            "└── VAL:\n",
            "    ├── cer: 467.6740\n",
            "    ├── wer: 623.4972\n",
            "    └── word_dist: 459.9000\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 1):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 2.1644\n",
            "│   ├── ctc_loss: 2.6061\n",
            "│   ├── joint_loss: 2.6856\n",
            "│   ├── perplexity_char: 8.7096\n",
            "│   └── perplexity_token: 8.7096\n",
            "└── VAL:\n",
            "    ├── cer: 366.0102\n",
            "    ├── wer: 501.6394\n",
            "    └── word_dist: 359.9333\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000230\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 2):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 2.0170\n",
            "│   ├── ctc_loss: 1.8383\n",
            "│   ├── joint_loss: 2.3847\n",
            "│   ├── perplexity_char: 7.5158\n",
            "│   └── perplexity_token: 7.5158\n",
            "└── VAL:\n",
            "    ├── cer: 103.0560\n",
            "    ├── wer: 155.1913\n",
            "    └── word_dist: 101.2333\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 3):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 1.5698\n",
            "│   ├── ctc_loss: 1.5423\n",
            "│   ├── joint_loss: 1.8782\n",
            "│   ├── perplexity_char: 4.8054\n",
            "│   └── perplexity_token: 4.8054\n",
            "└── VAL:\n",
            "    ├── cer: 46.3497\n",
            "    ├── wer: 79.2350\n",
            "    └── word_dist: 46.2667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 4):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 1.1743\n",
            "│   ├── ctc_loss: 1.3142\n",
            "│   ├── joint_loss: 1.4371\n",
            "│   ├── perplexity_char: 3.2357\n",
            "│   └── perplexity_token: 3.2357\n",
            "└── VAL:\n",
            "    ├── cer: 33.9559\n",
            "    ├── wer: 61.0200\n",
            "    └── word_dist: 34.3667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 5):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 1.0560\n",
            "│   ├── ctc_loss: 1.1678\n",
            "│   ├── joint_loss: 1.2895\n",
            "│   ├── perplexity_char: 2.8748\n",
            "│   └── perplexity_token: 2.8748\n",
            "└── VAL:\n",
            "    ├── cer: 27.4703\n",
            "    ├── wer: 51.3661\n",
            "    └── word_dist: 27.8333\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 6):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.9839\n",
            "│   ├── ctc_loss: 1.0587\n",
            "│   ├── joint_loss: 1.1956\n",
            "│   ├── perplexity_char: 2.6748\n",
            "│   └── perplexity_token: 2.6748\n",
            "└── VAL:\n",
            "    ├── cer: 23.0900\n",
            "    ├── wer: 45.5373\n",
            "    └── word_dist: 23.0667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 7):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.9338\n",
            "│   ├── ctc_loss: 0.9765\n",
            "│   ├── joint_loss: 1.1291\n",
            "│   ├── perplexity_char: 2.5442\n",
            "│   └── perplexity_token: 2.5442\n",
            "└── VAL:\n",
            "    ├── cer: 19.7284\n",
            "    ├── wer: 39.3443\n",
            "    └── word_dist: 20.7667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 8):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.8993\n",
            "│   ├── ctc_loss: 0.9086\n",
            "│   ├── joint_loss: 1.0810\n",
            "│   ├── perplexity_char: 2.4578\n",
            "│   └── perplexity_token: 2.4578\n",
            "└── VAL:\n",
            "    ├── cer: 26.4177\n",
            "    ├── wer: 44.8087\n",
            "    └── word_dist: 29.5667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000327\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 9):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.8648\n",
            "│   ├── ctc_loss: 0.8479\n",
            "│   ├── joint_loss: 1.0344\n",
            "│   ├── perplexity_char: 2.3745\n",
            "│   └── perplexity_token: 2.3745\n",
            "└── VAL:\n",
            "    ├── cer: 16.6723\n",
            "    ├── wer: 36.6120\n",
            "    └── word_dist: 16.3667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 10):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.8399\n",
            "│   ├── ctc_loss: 0.7961\n",
            "│   ├── joint_loss: 0.9991\n",
            "│   ├── perplexity_char: 2.3162\n",
            "│   └── perplexity_token: 2.3162\n",
            "└── VAL:\n",
            "    ├── cer: 15.2462\n",
            "    ├── wer: 31.8761\n",
            "    └── word_dist: 14.9667\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 11):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.8194\n",
            "│   ├── ctc_loss: 0.7557\n",
            "│   ├── joint_loss: 0.9705\n",
            "│   ├── perplexity_char: 2.2691\n",
            "│   └── perplexity_token: 2.2691\n",
            "└── VAL:\n",
            "    ├── cer: 14.0917\n",
            "    ├── wer: 28.9617\n",
            "    └── word_dist: 13.9333\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 12):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 0.8039\n",
            "│   ├── ctc_loss: 0.7224\n",
            "│   ├── joint_loss: 0.9484\n",
            "│   ├── perplexity_char: 2.2343\n",
            "│   └── perplexity_token: 2.2343\n",
            "└── VAL:\n",
            "    ├── cer: 13.5484\n",
            "    ├── wer: 27.1403\n",
            "    └── word_dist: 13.3000\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   3%|▎         | 138/4757 [01:18<41:17,  1.86it/s, acc_step=1/1, ce_loss=0.7936, ctc_loss=0.7285, joint_loss=0.9393, perplexity=2.2114]"
          ]
        }
      ],
      "source": [
        "trainer.train(train_loader, val_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9o7QclnXWX2"
      },
      "source": [
        "#### Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtZ3E46N0Kt"
      },
      "outputs": [],
      "source": [
        "# Define the recognition config: Greedy search\n",
        "recognition_config = {\n",
        "    'num_batches': None,\n",
        "    'temperature': 1.0,\n",
        "    'repeat_penalty': 1.0,\n",
        "    'lm_weight': None,\n",
        "    'lm_model': None,\n",
        "    'beam_width': 1, # Beam width of 1 reverts to greedy\n",
        "}\n",
        "\n",
        "# Recognize with the shallow fusion config\n",
        "config_name = \"test\"\n",
        "print(f\"Evaluating with {config_name} config\")\n",
        "results = trainer.recognize(test_loader, recognition_config, config_name=config_name, max_length=max_transcript_len)\n",
        "\n",
        "\n",
        "# Calculate metrics on full batch\n",
        "generated = [r['generated'] for r in results]\n",
        "results_df = pd.DataFrame(\n",
        "    {\n",
        "        'id': range(len(generated)),\n",
        "        'transcription': generated\n",
        "    }\n",
        ")\n",
        "\n",
        "# Cleanup (Will end wandb run)\n",
        "trainer.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srfle5jUeGYA"
      },
      "source": [
        "## Submit to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svtIbvZg3mdw"
      },
      "source": [
        "### Authenticate Kaggle\n",
        "In order to use the Kaggle’s public API, you must first authenticate using an API token. Go to the 'Account' tab of your user profile and select 'Create New Token'. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "- `TODO`: Set your kaggle username and api key here based on the API credentials listed in the kaggle.json\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uwDDX1Om3mdx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"ruthvikveeravalli\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"7155022e7d513887a13fc49944d0bc96\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jFRZqdazOfZt",
        "outputId": "e9a1bb08-fed7-4716-8ab9-b1669d3e3bab"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-17bb966b436f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ],
      "source": [
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ydl_jG3mdx"
      },
      "source": [
        "### Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bqWwBD6kO0Zf"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "!kaggle competitions submit -c 11785-s25-hw4p2-asr -f results.csv -m \"My Submission\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDd1int5Dxl2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RV797kwAXWXp",
        "dqzcpqG0XWXp",
        "uP0Aucc7XWXs",
        "k5ey-Nd0XWXx"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}